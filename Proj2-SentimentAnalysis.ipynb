{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmentWords(s): \n",
    "    return s.split()\n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for c in os.listdir(\"data_training\"):\n",
    "    directory = \"data_training/\" + c\n",
    "    if c.startswith('.'):\n",
    "        continue\n",
    "    for f in os.listdir(directory):\n",
    "        if f.startswith('.'):\n",
    "            continue\n",
    "        words = readFile(directory + \"/\" + f)\n",
    "        e = {x: words.count(x) for x in words}\n",
    "        e['_file'] = f\n",
    "        e['_class'] = c\n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0005</th>\n",
       "      <th>\u0013earth</th>\n",
       "      <th>\u0013goodies</th>\n",
       "      <th>\u0013if</th>\n",
       "      <th>\u0013ripley</th>\n",
       "      <th>\u0013suspend</th>\n",
       "      <th>\u0013they</th>\n",
       "      <th>\u0013white\u0014</th>\n",
       "      <th>\u0014</th>\n",
       "      <th>\u0016</th>\n",
       "      <th>...</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg's</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwick's</th>\n",
       "      <th>zwigoff's</th>\n",
       "      <th>zycie</th>\n",
       "      <th>zycie'</th>\n",
       "      <th>|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     \u0005  \u0013earth  \u0013goodies  \u0013if  \u0013ripley  \u0013suspend  \u0013they  \u0013white\u0014    \u0014    \u0016  \\\n",
       "0  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
       "1  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
       "2  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
       "3  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
       "4  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
       "\n",
       "  ...   zukovsky  zundel  zurg's  zweibel  zwick  zwick's  zwigoff's  zycie  \\\n",
       "0 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
       "1 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
       "2 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
       "3 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
       "4 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
       "\n",
       "   zycie'    |  \n",
       "0     0.0  0.0  \n",
       "1     0.0  0.0  \n",
       "2     0.0  0.0  \n",
       "3     0.0  0.0  \n",
       "4     0.0  0.0  \n",
       "\n",
       "[5 rows x 45673 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(d).fillna(value = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=.8)\n",
    "train_df=df.sample(frac=0.8, random_state=200)\n",
    "test_df=df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = list(set(df.columns) - {'_class', '_file'})\n",
    "X_train, y_train = train_df[columns], train_df._class\n",
    "X_test, y_test = test_df[columns], test_df._class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonregularized\n",
    "log_model = LogisticRegression(penalty='l2', C=10000)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83437499999999998"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01546583,  0.00401547, -0.03068038, ..., -0.02267432,\n",
       "        -0.00450413,  0.0028457 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regularized\n",
    "log_model = LogisticRegression(penalty='l2', C=1)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81874999999999998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00636629,  0.00194769, -0.01221163, ..., -0.00638716,\n",
       "        -0.00149716,  0.00346259]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.000229\n",
      "dtype: float64\n",
      "0    0.028671\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18153,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame(log_model.coef_)\n",
    "total_mean = np.mean(features, axis = 1)\n",
    "total_std = np.std(features, axis = 1)\n",
    "normal_features = features.subtract(total_mean, axis = 0) \\\n",
    "                          .divide(total_std, axis = 0)\n",
    "feat_selec = np.where(np.absolute(normal_features) > 0.3)[1]\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "feat_selec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selec = X_train.drop(X_train.columns[feat_selec], axis = 1)\n",
    "log_model = LogisticRegression(penalty = 'l2', C = 1)\n",
    "log_model.fit(X_train_selec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66249999999999998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_selec = X_test.drop(X_test.columns[feat_selec], axis = 1)\n",
    "log_model.score(X_test_selec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To select which features to eliminate, we eliminated all features of which the absolute value of the normalized feature is greater than 0.3 standard deviations away from the mean. We used this to eliminate any features which either would overfit or underfit the data to the training data, in order to make it fit the test data more effectively. Eventually, it turned out that there was a loss of 1% in accuracy, but there was a reduction of about 13000 features, or slightly over 25% of the features, making this a simpler model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tree = DecisionTreeClassifier()\n",
    "d_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into sklearn.model_selection.GridSearchCV\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 2 0.1 1\n"
     ]
    }
   ],
   "source": [
    "best_score_parameter_i = 0\n",
    "best_score_i = 0\n",
    "for i in range(-10, -4):\n",
    "    d_tree = DecisionTreeClassifier(min_impurity_split= 10 ** i)\n",
    "    d_tree = d_tree.fit(X_train, y_train)\n",
    "    score = d_tree.score(X_test, y_test)\n",
    "    if score > best_score_i:\n",
    "        best_score_parameters_i = 10 ** i\n",
    "        best_score_i = score\n",
    "best_score_parameter_j = 0\n",
    "best_score_j = 0\n",
    "for j in range(1, 5):\n",
    "    d_tree = DecisionTreeClassifier(min_samples_leaf= j)\n",
    "    d_tree = d_tree.fit(X_train, y_train)\n",
    "    score = d_tree.score(X_test, y_test)\n",
    "    if score > best_score_j:\n",
    "        best_score_parameter_j = j\n",
    "        best_score_j = score\n",
    "best_score_parameter_k = 0\n",
    "best_score_k = 0\n",
    "for k in range(2, 6):\n",
    "    d_tree = DecisionTreeClassifier(min_samples_split= k)\n",
    "    d_tree = d_tree.fit(X_train, y_train)\n",
    "    score = d_tree.score(X_test, y_test)\n",
    "    if score > best_score_k:\n",
    "        best_score_parameter_k = k\n",
    "        best_score_k = score\n",
    "best_score_parameter_l = 0\n",
    "best_score_l = 0\n",
    "for l in range(5):\n",
    "    d_tree = DecisionTreeClassifier(min_weight_fraction_leaf= l * 1.0 / 10)\n",
    "    d_tree = d_tree.fit(X_train, y_train)\n",
    "    score = d_tree.score(X_test, y_test)\n",
    "    if score > best_score_l:\n",
    "        best_score_parameter_l = l * 1.0 / 10\n",
    "        best_score_l = score\n",
    "best_score_parameter_m = 0\n",
    "best_score_m = 0\n",
    "for m in range(2):\n",
    "    d_tree = DecisionTreeClassifier(presort=m)\n",
    "    d_tree = d_tree.fit(X_train, y_train)\n",
    "    score = d_tree.score(X_test, y_test)\n",
    "    if score > best_score_m:\n",
    "        best_score_parameter_m = m\n",
    "        best_score_m = score\n",
    "d_tree\n",
    "\n",
    "print(best_score_parameter_i,\n",
    "      best_score_parameter_j,\n",
    "      best_score_parameter_k,\n",
    "      best_score_parameter_l,\n",
    "      best_score_parameter_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640625\n"
     ]
    }
   ],
   "source": [
    "d_tree = DecisionTreeClassifier(min_impurity_split = best_score_parameter_i,\n",
    "                                min_samples_leaf = best_score_parameter_j,\n",
    "                                min_samples_split = best_score_parameter_k,\n",
    "                                min_weight_fraction_leaf = best_score_parameter_l,\n",
    "                                presort = best_score_parameter_m)\n",
    "d_tree = d_tree.fit(X_train, y_train)\n",
    "best_score = d_tree.score(X_test, y_test)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I used a for loop to run through the range of reasonable parameters, and individually picked out the best ones.\n",
    "I changed all the parameters that had a numerical or boolean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because decision trees can continue splitting up the parameter space to better match the data, it means they\n",
    "will not stop until they have the perfect fitting unless specifically told otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68437499999999996"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "rf1.fit(X_train, y_train)\n",
    "rf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2 = sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "\n",
    "rf2.fit(X_train, y_train)\n",
    "rf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I increased `num_estimators`, or the number of trees in the forest, since more trees should lead to more reliable results that react better to outliers. In addition, I changed the `criterion` to entropy, since that is the criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While the predictions of a single decision tree is highly sensitive to noise in the training set, aggregating the results of many uncorrelated trees makes this much less likely. In this way, using random forests significantly decreases the variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
